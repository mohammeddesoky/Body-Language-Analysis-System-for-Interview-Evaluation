# ğŸ§  Body Language Analysis System for Interview Evaluation

This project is a real-time computer vision system that analyzes non-verbal behaviors in interviews to provide behavioral feedback. It detects **facial emotions**, **eye gaze**, **head pose**, and **hand-to-face gestures** using computer vision and deep learning.

---

## ğŸ“‚ Modules Overview

| Module | Purpose |
|--------|---------|
| ğŸ‘ï¸ Eye Tracking & Blink Detection | Tracks eye gaze (left, center, right) and detects blinking using facial landmarks and EAR. |
| ğŸ˜ Emotion Recognition | Uses EmotiEffNet (ONNX) to classify facial expressions in real-time. |
| ğŸ§­ Head Pose Estimation | Estimates head orientation (pitch, yaw) and determines direction of gaze. |
| âœ‹ğŸ¤š Hand-to-Face Detection | Detects hand touching the face using distance between hand and face landmarks. |
| ğŸš¨ Behavioral Alert System | Triggers alerts based on sustained behaviors like staring, face-touching, or static emotion. |

---

## ğŸ› ï¸ Features

### ğŸ‘ï¸ Eye Tracking and Blink Detection

- Uses MediaPipe face mesh and iris landmarks.
- **Blink Detection** using Eye Aspect Ratio (EAR).
- **Gaze Direction** using averaged eye and iris positions:
  - Eye center = average of eye boundary landmarks.
  - Iris center = average of iris landmarks.
  - Compares position to determine if user is looking **left**, **right**, or **center**.

### ğŸ˜ Facial Emotion Recognition

- Uses **EmotiEffNet (ONNX)** for real-time facial expression classification.
- Emotions detected: *Happy, Sad, Angry, Neutral*, etc.
- Integrated with **MediaPipe** face detection for fast ROI extraction.

### ğŸ§­ Head Pose Estimation

- Estimates head orientation using OpenCVâ€™s `solvePnP()` from 2D-3D landmark mapping.
- Euler angles (pitch, yaw) used to classify head pose:
  - Yaw < â€“10Â° â†’ Looking Left
  - Yaw > +10Â° â†’ Looking Right
  - Pitch > +10Â° â†’ Looking Up
  - Pitch < â€“10Â° â†’ Looking Down
  - Near zero â†’ Looking Forward

### âœ‹ğŸ¤š Hand-to-Face Gesture Detection

- Detects hand proximity to the face using MediaPipe hand and face landmarks.
- If a hand landmark lies within Â±30 pixels of any facial landmark â†’ Hand touching face.
- Interpreted as **stress** or **nervousness**.

### ğŸš¨ Behavioral Alert System

Triggers alerts when a condition is sustained for more than **10 seconds**:

| Behavior | Condition | Interpretation |
|----------|-----------|----------------|
| Hand on Face | Contact >10s | Nervousness or discomfort |
| Fixed Head Pose | Static >10s | Fatigue or disengagement |
| Fixed Gaze | Centered >10s | Zoning out or boredom |
| Neutral Emotion | No change >10s | Low engagement |

---

## ğŸ“Š Result of Feedback

### ğŸ§¾ 1. Behavior Summary Table (Every 10 seconds)

Summarizes real-time behavior logs:

![Behavior Summary](results/feedback_summary1.png)
![Behavior Summary](results/feedback_summary2.png)


